import numpy as np
from tf_agents.environments.py_environment import PyEnvironment
from tf_agents.specs.array_spec import ArraySpec, BoundedArraySpec
from tf_agents.trajectories import time_step as ts


class ClassifierEnv(PyEnvironment):
    """
    Custom `PyEnvironment` environment for imbalanced classification.
    Based on https://www.tensorflow.org/agents/tutorials/2_environments_tutorial
    """

    def __init__(self, X_train: np.ndarray, y_train: np.ndarray):
        """Initialization of environment with X_train and y_train.

        :param X_train: Features shaped: [samples, ..., ]
        :type  X_train: np.ndarray
        :param y_train: Labels shaped: [samples]
        :type  y_train: np.ndarray

        :returns: None
        :rtype: NoneType

        """
        self._action_spec = BoundedArraySpec(shape=(), dtype=np.int32, minimum=0, maximum=8, name="action") # Change this.
        self._observation_spec = ArraySpec(shape=X_train.shape[1:], dtype=X_train.dtype, name="observation")
        self._episode_ended = False

        self.X_train = X_train
        self.y_train = y_train
        self.id = np.arange(self.X_train.shape[0])  # List of IDs to connect X and y data

        self.episode_step = 0  # Episode step, resets every episode
        self._state = self.X_train[self.id[self.episode_step]]

    def action_spec(self):
        """
        Definition of the discrete actionspace.
        1 for the positive/minority class, 0 for the negative/majority class.
        """
        return self._action_spec

    def observation_spec(self):
        """Definition of the continous statespace e.g. the observations in typical RL environments."""
        return self._observation_spec

    def _reset(self):
        """Shuffles data and returns the first state of the shuffled data to begin training on new episode."""
        np.random.shuffle(self.id)  # Shuffle the X and y data
        self.episode_step = 0  # Reset episode step counter at the end of every episode
        self._state = self.X_train[self.id[self.episode_step]]
        self._episode_ended = False  # Reset terminal condition

        return ts.restart(self._state)

    def _step(self, action: int):
        """
        Take one step in the environment.
        If the action is correct, the environment will either return 1 or `imb_ratio` depending on the current class.
        If the action is incorrect, the environment will either return -1 or -`imb_ratio` depending on the current class.
        """
        if self._episode_ended:
            # The last action ended the episode. Ignore the current action and start a new episode
            return self.reset()

        env_action = self.y_train[self.id[self.episode_step]]  # The label of the current state
        self.episode_step += 1

        # {'backdoor': 0, 'banker': 1, 'cryptominer': 2, 'deceptor': 3, 'downloader': 4, 'normal': 5, 
        #  'pua': 6, 'ransomware': 7, 'spyware': 8}

        # To get reward, do rewards[acutal][predicted].
        rewards = {
            0: {0: 1, 1: -2, 2: -2, 3: -2, 4: -1, 5: -2, 6: -2, 7: -2, 8: -1},
            1: {0: -1, 1: 1, 2: -2, 3: -1, 4: -2, 5: -2, 6: -2, 7: -2, 8: -1},
            2: {0: -2, 1: -2, 2: 1, 3: -2, 4: -2, 5: -2, 6: -2, 7: -2, 8: -2},
            3: {0: -2, 1: -2, 2: -2, 3: 1, 4: -1, 5: -2, 6: -2, 7: -2, 8: -2},
            4: {0: -2, 1: -2, 2: -2, 3: -1, 4: 1, 5: -2, 6: -2, 7: -2, 8: -2},
            5: {0: -2, 1: -2, 2: -2, 3: -2, 4: -2, 5: 1, 6: -2, 7: -2, 8: -2},
            6: {0: -1, 1: -2, 2: -2, 3: -1, 4: -1, 5: -2, 6: 1, 7: -2, 8: -2},
            7: {0: -2, 1: -2, 2: -2, 3: -2, 4: -2, 5: -2, 6: -2, 7: 1, 8: -2},
            8: {0: -1, 1: -1, 2: -2, 3: -2, 4: -1, 5: -2, 6: -2, 7: -2, 8: 1},
        }
        # rewards = {
        #     0: {0: 1, 1: -2, 2: -2, 3: -2, 4: -1, 5: -2, 6: -2, 7: -2, 8: -1},
        #     1: {0: -1, 1: 1, 2: -2, 3: -1, 4: -2, 5: -2, 6: -2, 7: -2, 8: -1},
        #     2: {0: -2, 1: -2, 2: 1, 3: -2, 4: -2, 5: -2, 6: -2, 7: -2, 8: -2},
        #     3: {0: -2, 1: -2, 2: -2, 3: 1, 4: -1, 5: -2, 6: -2, 7: -2, 8: -2},
        #     4: {0: -2, 1: -2, 2: -2, 3: -1, 4: 1, 5: -2, 6: -2, 7: -2, 8: -2},
        #     5: {0: -2, 1: -2, 2: -2, 3: -2, 4: -2, 5: 1, 6: -2, 7: -2, 8: -2},
        #     6: {0: -1, 1: -2, 2: -2, 3: -1, 4: -1, 5: -2, 6: 1, 7: -2, 8: -2},
        #     7: {0: -2, 1: -2, 2: -2, 3: -2, 4: -2, 5: -2, 6: -2, 7: 1, 8: -2},
        #     8: {0: -1, 1: -1, 2: -2, 3: -2, 4: -1, 5: -2, 6: -2, 7: -2, 8: 1},
        # }
        # if action == env_action:  # Correct action
        #     reward = 1
        # else:  # Incorrect action
        #     reward = 0
    
        reward = rewards[int(env_action)][int(action)]

        if self.episode_step == self.X_train.shape[0] - 1:  # If last step in data
            self._episode_ended = True

        self._state = self.X_train[self.id[self.episode_step]]  # Update state with new datapoint

        if self._episode_ended:
            return ts.termination(self._state, reward)
        else:
            return ts.transition(self._state, reward)
